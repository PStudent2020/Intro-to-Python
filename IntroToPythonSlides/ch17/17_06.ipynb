{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructor Notes:**\n",
    "* This notebook should be executed in the  **`jupyter/pyspark-notebook`** Docker stack, which is configured with **Spark and PySpark** \n",
    "* The notes on doing that are provided in this notebook and in the book chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable high-res images in notebook \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.6 Spark\n",
    "* Use **PySpark** and **Spark functional-style programming** to summarize **word frequencies** in **Romeo and Juliet**\n",
    "* **Hadoop** break tasks into pieces that do **lots of disk I/O across many computers**\n",
    "* **Spark** performs certain big-data tasks **in memory** for **better performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.6.1 Spark Overview\n",
    "* In big data, **performance is crucial**\n",
    "* **Hadoop** is geared to **disk-based** batch processing\n",
    "    * read data from disk\n",
    "    * process the data\n",
    "    * write results back to disk\n",
    "* Many **big-data** applications **demand better performance** \n",
    "    * E.g., **fast streaming** applications requiring **real-time** or **near-real-time processing** won’t work in a **disk-based architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and Components \n",
    "* Its **in-memory** architecture “**has been used to sort 100 TB of data 3X faster than Hadoop MapReduce on 1/10th of the machines**”[\\[2\\]](https://spark.apache.org/faq.html) \n",
    "* Runs some workloads up to **100 times faster** than Hadoop [\\[3\\]](https://spark.apache.org/) \n",
    "* Spark uses **resilient distributed datasets (RDDs)** to process distributed data with **functional-style programming** \n",
    "* Hadoop uses **replication for fault tolerance** &mdash; adds **more disk-based overhead**\n",
    "* **RDDs** eliminate disk-based overhead by \n",
    "    * **remaining in memory** &mdash; use disk only if data **can't fit in memory**\n",
    "    * **not replicating data**\n",
    "* **Fault tolerance** &mdash; Spark **remembers steps** used to **create an RDD**\n",
    "    * If a **node fails**, Spark **rebuilds the RDD** [\\[1\\]](https://spark.apache.org/research.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and Components (cont.)\n",
    "* **Spark distributes operations** to a cluster’s nodes for **parallel execution**\n",
    "* **Spark streaming** enables you to **process data as it’s received**\n",
    "* **Spark `DataFrame`s** (similar to pandas `DataFrames`), enable you to **manipulate RDDs** as a **collection of named columns**\n",
    "* Can use **Spark `DataFrame`s** with **Spark SQL** to **query distributed data**\n",
    "* **Spark MLlib** (the **Spark Machine Learning Library**) enables you to perform **machine-learning algorithms** on distributed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providers\n",
    "* Hadoop providers typicalluy also provide Spark support\n",
    "* [**Databricks**](https://databricks.com)\n",
    "    * A **Spark-specific vendor**  \n",
    "    * Their website is an excellent resource for **learning Spark**\n",
    "    * **Paid version** runs on **Amazon AWS** or **Microsoft Azure**\n",
    "    * **Free Databricks Community Edition** is a great way to get started with both **Spark** and the **Databricks** environment\n",
    "    * [**Databricks free e-books**](https://databricks.com/resources/type/ebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.6.2 Docker and the Jupyter Docker Stacks\n",
    "### Docker \n",
    "* **Docker** is a **tool for packaging software** into **containers** (also called **images**) that **bundle everything required to execute that software across platforms**.\n",
    "* Some software packages we use in this chapter require **complicated setup and configuration**.\n",
    "* For many of these, there are **preexisting Docker containers** that you can **download for free and execute locally** on your desktop or notebook computers.\n",
    "* These help you **get started with new technologies quickly and conveniently**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker (cont.)\n",
    "* Docker also helps with **reproducibility** in research and analytics studies.\n",
    "* You can create custom Docker containers that are configured with the versions of every piece of software and every library you used in your study.\n",
    "* This would enable others to **recreate the environment you used**, then **reproduce your work**, and will help you **reproduce your results at a later time**.\n",
    "* We’ll use Docker in this section to download and execute a Docker container that’s preconfigured to run Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Docker\n",
    "* You can install [Docker for Windows 10 Pro or macOS as described here](https://www.docker.com/products/docker-desktop)\n",
    "\t* On Windows 10 Pro, you must allow the `\"Docker for Windows.exe\"` installer to make changes to your system to complete the installation process. To do so, click **Yes** when Windows asks if you want to allow the installer to make changes to your system. \n",
    "\t\t* Some Windows users might have to follow the instructions under **Allow specific apps to make changes to controlled folders** at https://docs.microsoft.com/en-us/windows/security/threat-protection/windows-defender-exploit-guard/customize-controlled-folders-exploit-guard.\n",
    "\t[Windows 10 Home users must use Virtual Box as described here](https://docs.docker.com/machine/drivers/virtualbox/)\n",
    "* Linux users should install [Docker Community Edition as described here](https://docs.docker.com/install/overview/)\n",
    "* For a general overview of Docker, read the [**Getting started** guide](https://docs.docker.com/get-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Docker Stacks \n",
    "* The Jupyter Notebooks team has **preconfigured several Jupyter “Docker stacks”** containers for common Python development scenarios.\n",
    "* Each enables you to use Jupyter Notebooks to experiment with powerful capabilities without having to worry about complex software setup issues.\n",
    "* In each case, you can open **JupyterLab in your web browser**, open a notebook in JupyterLab and start coding.\n",
    "* JupyterLab also provides a **Terminal window** that you can use in your browser like your computer’s Terminal, Anaconda Command Prompt or shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Docker Stacks (cont.)\n",
    "* We’ll use the **`jupyter/pyspark-notebook` Docker stack**, which is **preconfigured with everything you need to create and test Apache Spark apps on your computer**.\n",
    "* When combined with installing other Python libraries we’ve used throughout the book, you can implement most of this book’s examples using this container.\n",
    "* [More about the **available Docker stacks**](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Jupyter Docker Stack \n",
    "* Before performing the next step, ensure that **JupyterLab** is not currently running on your computer.\n",
    "* Let’s download and run the **`jupyter/pyspark-notebook` Docker stack**.\n",
    "* To ensure that you do not lose your work when you close the Docker container, we’ll attach a local file-system folder to the container and use it to save your notebook—Windows users should replace `\\` with `^`\n",
    "    * Note: You should replace \"fullPathToTheFolderYouWantToUse\" with the actual full path of a folder on your system&mdash;in this case, the one containing the ch17 examples\n",
    ">```\n",
    "docker run -p 8888:8888 -p 4040:4040 -it --user root \\\n",
    "    -v fullPathToTheFolderYouWantToUse:/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook:14fdfbf9cfc1 start.sh jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Jupyter Docker Stack (1 of 2)\n",
    "* The first time you run the preceding command, Docker will download the Docker container named: \n",
    "```\n",
    "jupyter/pyspark-notebook:14fdfbf9cfc1\n",
    "```\n",
    "* The notation `\":14fdfbf9cfc1\"` indicates the specific `jupyter/pyspark-notebook` container to download.\n",
    "* At the time of this writing, `14fdfbf9cfc1` was the newest version of the container.\n",
    "* Specifying the version as we did here helps with **reproducibility**.\n",
    "* Without the `\":14fdfbf9cfc1\"` in the command, Docker will download the latest version of the container, which might contain different software versions and might not be compatible with the code you’re trying to execute.\n",
    "* The Docker container is nearly 6GB, so the initial download time will depend on your Internet connection’s speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Docker Container’s Command Line (1 of 2)\n",
    "* Each **Docker container has a command-line interface** like the one you’ve used to run IPython throughout this book.\n",
    "* Via this interface, you **can install Python packages into the Docker container** and even **use IPython** as you’ve done previously. \n",
    "* Open a separate Anaconda Command Prompt, Terminal or shell and list the currently running Docker containers with the command:\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "* The output of this command is wide, so the lines of text will likely wrap, as in:\n",
    "\n",
    "```\n",
    "CONTAINER ID        IMAGE                                   COMMAND  \n",
    "           CREATED             STATUS            PORTS             \n",
    "  NAMES\n",
    "f54f62b7e6d5        jupyter/pyspark-notebook:14fdfbf9cfc1   \"tini -g -- \n",
    "/bin/bash\"  2 minutes ago      Up 2 minutes      0.0.0.0:8888->8888/tcp\n",
    "  friendly_pascal\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Docker Container’s Command Line (2 of 2)\n",
    "* In the last line of our system’s output under the column head `NAMES` in the third line is the name that Docker randomly assigned to the **running container**—`friendly_pascal`—**the name on your system will differ**\n",
    "* To access the **container’s command line**, execute the following command, replacing **container_name** with the running container’s name:\n",
    "```\n",
    "docker exec -it container_name /bin/bash\n",
    "```\n",
    "* The **Docker container** uses Linux under the hood, so you’ll see a Linux prompt where you can enter commands. \n",
    "* The app in this section will use features of the **NLTK and TextBlob** libraries you used in the “Natural Language Processing” chapter.\n",
    "* Neither is preinstalled in the **Jupyter Docker stacks**.\n",
    "* To install NLTK and TextBlob enter the command:\n",
    "```\n",
    "conda install -c conda-forge nltk textblob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping and Restarting a Docker Container\n",
    "* Every time you start a container with `docker run`, Docker gives you a new instance that does not contain any libraries you installed previously.\n",
    "* For this reason, you should keep track of your **container name**, so you can use it from another command-line window to stop the container and restart it.\n",
    "* The following command will shut down the container\n",
    ">```\n",
    "docker stop your_container_name\n",
    "``` \n",
    "* The following command will restart the specified container \n",
    ">```\n",
    "docker restart your_container_name\n",
    "``` \n",
    "* **Docker** also provides a **GUI app called Kitematic** that you can use to manage your containers, including stopping and restarting them.\n",
    "* You can get the app from https://kitematic.com/ and access it through the Docker menu.\n",
    "* [The user guide overviews how to **manage containers** with the tool](https://docs.docker.com/kitematic/userguide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.6.3 Word Count with Spark\n",
    "* Use Spark **filter, map and reduce** to implement a simple **word count** example that summarizes the words in **Romeo and Juliet**\n",
    "\n",
    "### Loading the NLTK Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a SparkContext \n",
    "* A **`SparkContext`** object gives you access to Spark’s capabilities\n",
    "* Some Spark environments **create a `SparkContext` for you** but not the Jupyter Docker stack\n",
    "* To create a **`SparkContext`**\n",
    "    * Specify the **configuration options** with a **`SparkConf`** object \n",
    "    * **`setMaster`** specifies the **Spark cluster’s URL**\n",
    "    * **`local[*]`** &mdash; Spark is executing on your **`local` computer** \n",
    "    * **`*`** &mdash; Use same number of **threads** as **cores** on your computer\n",
    "        * Simulates **parallelism of Spark clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "configuration = SparkConf().setAppName('RomeoAndJulietCounter')\\\n",
    "                           .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Text File and Mapping It to Words\n",
    "* You work with a **`SparkContext`** using **functional-style programming** applied to an **RDD**\n",
    "* **RDD** enables you to **transform the data** stored throughout a **cluster** in **HDFS**\n",
    "* Get a new **`RDD`** representing all words in **Romeo and Juliet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.utils import strip_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sc.textFile('RomeoAndJuliet.txt')\\\n",
    "              .flatMap(lambda line: line.lower().split())\\\n",
    "              .map(lambda word: strip_punc(word, all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the Stop Words\n",
    "* Get a new **`RDD`** with **no stop words** remaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = tokenized.filter(lambda word: word not in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Each Remaining Word \n",
    "* Now we can **count** the number of **occurrences** of each word\n",
    "* First **`map`** each word to a **tuple** containing the **word** and **`1`**\n",
    "* **`reduceByKey`** with the **`operator`** module’s **`add` function** as an argument **adds** the counts for tuples that contain same **key** (`word`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "word_counts = filtered.map(lambda word: (word, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Only the Words with Counts Greater Than or Equal to 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_counts = word_counts.filter(lambda item: item[1] >= 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting in Descending Order and Displaying the Results \n",
    "* At this point, we’ve specified all the steps to **count the words**\n",
    "* When you call an **`RDD`'s `collect` method**, **Spark** \n",
    "    * initiates the **processing steps**\n",
    "    * **returns a list** containing the final results &mdash; **word-count tuples**\n",
    "* Everything **appears to execute on one computer**\n",
    "* Spark **distributes tasks among the cluster’s worker nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_items = sorted(filtered_counts.collect(),\n",
    "                      key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and Displaying the Results (cont.)\n",
    "* We determine the **word with the most letters** so we can **right-align** the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   romeo: 298\n",
      "    thou: 277\n",
      "  juliet: 178\n",
      "     thy: 170\n",
      "   nurse: 146\n",
      " capulet: 141\n",
      "    love: 136\n",
      "    thee: 135\n",
      "   shall: 110\n",
      "    lady: 109\n",
      "   friar: 104\n",
      "    come: 94\n",
      "mercutio: 83\n",
      "    good: 80\n",
      "benvolio: 79\n",
      "   enter: 75\n",
      "      go: 75\n",
      "    i’ll: 71\n",
      "  tybalt: 69\n",
      "   death: 69\n",
      "   night: 68\n",
      "lawrence: 67\n",
      "     man: 65\n",
      "    hath: 64\n",
      "     one: 60\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(word) for word, count in sorted_items])\n",
    "for word, count in sorted_items:\n",
    "    print(f'{word:>{max_len}}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminate current SparkContext so we can create another for next example\n",
    "sc.stop()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Monitoring Interface\n",
    "* https://spark.apache.org/docs/latest/monitoring.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.6.4 Spark Word Count on Microsoft Azure\n",
    "* In this section, you’ll implement the **Spark word-count example** on a **Microsoft Azure HDInsight Spark cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Apache Spark Cluster in HDInsight Using the Azure Portal\n",
    "* [How to set up a **Spark cluster** using the **HDInsight service**](https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-portal)\n",
    "* While following the **Create an HDInsight Spark cluster** steps, note the same issues we listed in the Hadoop cluster setup earlier\n",
    "* For the **Cluster type** select **Spark**\n",
    "* Default cluster configuration provides more resources than you need.\n",
    "* In the **Cluster summary**, perform the steps shown in the Hadoop cluster setup to change the number of worker nodes to 2 and to configure the worker and head nodes to use **D3 v2** computers\n",
    "* When you click **Create**, it takes 20 to 30 minutes to configure and deploy your cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries into a Cluster \n",
    "* If your Spark code requires libraries that are not installed in the HDInsight cluster, you’ll need to install them\n",
    "* To see what libraries are installed by default, you can use `ssh` to log into your cluster (as shown earlier) and execute the command:\n",
    ">```\n",
    "/usr/bin/anaconda/envs/py35/bin/conda list\n",
    "```\n",
    "* Since your code will execute on multiple cluster nodes, libraries must be installed on **every** node\n",
    "* Azure requires you to create a Linux shell script that specifies the commands to install the libraries\n",
    "* When you submit that script to Azure, it validates the script, then executes it on every node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries into a Cluster (cont.)\n",
    "* Linux shell scripts are beyond this book’s scope, and the script must be hosted on a web server from which Azure can download the file\n",
    "* So, we created an install script for you that installs the libraries we use in the Spark examples\n",
    "* Perform the following steps to install these libraries:\n",
    "\t1. In the Azure portal, select your cluster.\n",
    "\t2. In the list of items under the cluster’s search box, click **Script Actions**.\n",
    "\t3. Click **Submit new** to configure the options for the library installation script. For the **Script type** select **Custom**, for the **Name** specify `libraries` and for the **Bash script URI** use: \n",
    "`http://deitel.com/bookresources/IntroToPython/install_libraries.sh`\n",
    "\t4. Check both **Head** and **Worker** to ensure that the script installs the libraries on all the nodes.\n",
    "\t5. Click **Create**. \n",
    "* When the cluster finishes executing the script, if it executed successfully, you’ll see a green check next to the script name in the list of script actions.\n",
    "* Otherwise, Azure will notify you that there were errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying `RomeoAndJuliet.txt` to the HDInsight Cluster \n",
    "* As you did in the Hadoop demo, use `scp` to upload `RomeoAndJuliet.txt` \n",
    ">```\n",
    "scp RomeoAndJuliet.txt sshuser@YourClusterName-ssh.azurehdinsight.net:\n",
    "```\n",
    "* Replace **YourClusterName** with the name you specified when creating your cluster and press **Enter** only when you’ve typed the entire command\n",
    "* The colon is required and indicates that you’ll supply your cluster password when prompted\n",
    "* At that prompt, type the password you specified when setting up the cluster, then press **Enter**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying `RomeoAndJuliet.txt` to the HDInsight Cluster (cont.)\n",
    "* Use `ssh` to log into your cluster and access its command line\n",
    ">```\n",
    "ssh sshuser@YourClusterName-ssh.azurehdinsight.net \n",
    "```\n",
    "* Replace **YourClusterName** with your cluster name.\n",
    "* Again, you’ll be prompted for your cluster password\n",
    "* To work with the `RomeoAndJuliet.txt` file in Spark, use `ssh` to copy the file into the cluster’s Hadoop’s file system by executing the following command\n",
    ">```\n",
    "hadoop fs -copyFromLocal RomeoAndJuliet.txt \n",
    "    /example/data/RomeoAndJuliet.txt\n",
    "```\n",
    "* We use the already existing folder `/examples/data` that Microsoft includes for use with HDInsight tutorials\n",
    "* Again, press Enter only when you’ve typed the entire command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Jupyter Notebooks in HDInsight\n",
    "* At the time of this writing, HDInsight uses the **old** Jupyter Notebook interface, rather than the newer JupyterLab interface shown earlier.\n",
    "* [A quick overview of the old interface](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Notebook%20Basics.html) \n",
    "* To access Jupyter Notebooks in HDInsight, in the Azure portal select **All resources**, then your cluster\n",
    "* In the **Overview** tab, select **Jupyter notebook** under **Cluster dashboards**\n",
    "* Opens a web browser window and asks you to log in\n",
    "* Use the username and password you specified when setting up the cluster\n",
    "    * If you did not specify a username, the default is `admin`.\n",
    "* Once you log in, Jupyter displays a folder containing `PySpark` and `Scala` subfolders\n",
    "* These contain Python and Scala Spark tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the RomeoAndJulietCounter.ipynb Notebook\n",
    "* You can create new notebooks by clicking **New** and selecting PySpark3, or you can upload existing notebooks from your computer.\n",
    "* For this example, let’s upload the previous section’s `RomeoAndJulietCounter.ipynb` notebook and modify it to work with Azure.\n",
    "* To do so, click the **Upload** button, navigate to the `ch17` example folder’s `SparkWordCount` folder, select `RomeoAndJulietCounter.ipynb` and click **Open**\n",
    "* Displays the file in the folder with an **Upload** button to its right\n",
    "    * Click that button to place the notebook in the current folder\n",
    "* Next, click the notebook’s name to open it in a new browser tab\n",
    "* Jupyter will display a **Kernel not found** dialog\n",
    "* Select **PySpark3** and click **OK**\n",
    "* **Do not run any cells yet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Notebook to Work with Azure \n",
    "* Perform the following steps, executing each cell as you complete the step\n",
    "1. The HDInsight cluster will not allow NLTK to store the downloaded stop words in NLTK’s default folder because it’s part of the system’s protected folders\n",
    "    * In the first cell, modify the call `nltk.download('stopwords')` as follows to store the stop words in the current folder (`'.'):\n",
    ">```python\n",
    "    nltk.download('stopwords', download_dir='.')\n",
    "```\n",
    "* When you execute the first cell, `Starting Spark application` appears below the cell while **HDInsight sets up a `SparkContext` object named `sc` for you**\n",
    "* When this task is complete, the cell’s code executes and downloads the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Notebook to Work with Azure (cont.)\n",
    "2. In the second cell, before loading the stop words, you must tell NLTK that they’re located in the current folder\n",
    "    * Add the following statement after the `import` statement to tell NLTK to search for its data in the current folder:\n",
    ">```python\n",
    "    nltk.data.path.append('.')\n",
    "```\n",
    "3. Because HDInsight sets up the `SparkContext` object for you, the third and fourth cells of the original notebook are not needed, so you can delete them\n",
    "    * Click inside it and select **Delete Cells** from Jupyter’s **Edit** menu, or click in the white margin to the cell’s left and type `dd` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Notebook to Work with Azure (cont.)\n",
    "4. In the next cell, specify the location of `RomeoAndJuliet.txt` in the underlying Hadoop file system\n",
    "    * Replace the string `'RomeoAndJuliet.txt'` with the string \n",
    ">```python\n",
    "    'wasb:///example/data/RomeoAndJuliet.txt'\n",
    "```\n",
    "    * The notation `wasb:/// `indicates that `RomeoAndJuliet.txt` is stored in a **Windows Azure Storage Blob (WASB)—Azure’s interface to the HDFS file system**\n",
    "5. **Azure currently uses Python 3.5.x, so it does not support f-strings** \n",
    "    * In the last cell, replace the f-string with the following older-style Python string formatting using the string method **`format`**:\n",
    ">```python\n",
    "    print('{:>{width}}: {}'.format(word, count, width=max_len))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Notebook to Work with Azure (cont.)\n",
    "* You’ll see the same final results as in the previous section\n",
    "* **Caution: Be sure to delete your cluster and other resources when you’re done with them, so you do not incur charges** \n",
    ">https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-portal\n",
    "* When you delete your Azure resources, **your notebooks will be deleted as well**\n",
    "    * You can download the notebook you just executed by selecting **File > Download as > Notebook (.ipynb)** in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "&copy;1992&ndash;2020 by Pearson Education, Inc. All Rights Reserved. This content is based on Chapter 5 of the book [**Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud**](https://amzn.to/2VvdnxE).\n",
    "\n",
    "DISCLAIMER: The authors and publisher of this book have used their \n",
    "best efforts in preparing the book. These efforts include the \n",
    "development, research, and testing of the theories and programs \n",
    "to determine their effectiveness. The authors and publisher make \n",
    "no warranty of any kind, expressed or implied, with regard to these \n",
    "programs or to the documentation contained in these books. The authors \n",
    "and publisher shall not be liable in any event for incidental or \n",
    "consequential damages in connection with, or arising out of, the \n",
    "furnishing, performance, or use of these programs.                  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
