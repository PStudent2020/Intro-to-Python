{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.5 Hadoop\n",
    "* Apache Hadoop&mdash;a key big-data infrastructure technology that also serves as the foundation for many recent advancements in big-data processing and an entire ecosystem of software tools that are continually evolving to support today’s big-data needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.1 Hadoop Overview \n",
    "* When Google was launched in 1998, the amount of online data was already enormous with [approximately 2.4 million websites](http://www.internetlivestats.com/total-number-of-websites/)—truly big data.\n",
    "* Today there are now [nearly two billion websites](http://www.internetlivestats.com/total-number-of-websites/) (almost a thousandfold increase) and Google is handling over [two trillion searches per year](http://www.internetlivestats.com/google-search-statistics/)! Having used Google search since its inception, our sense is that today’s responses are significantly faster.\n",
    "* When Google was developing their search engine, they knew that they needed to return search results quickly.\n",
    "* The only practical way to do this was to store and index the entire Internet using a clever combination of secondary storage and main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.1 Hadoop Overview (cont.)\n",
    "* Computers of that time couldn’t hold that amount of data economically and could not analyze that amount of data fast enough to guarantee prompt search-query responses.\n",
    "* So Google developed a **clustering** system, tying together vast numbers of commodity computers—called **nodes**.\n",
    "* Because having more computers and more connections between them meant greater chance of hardware failures, they also built in high levels of **redundancy** to ensure that the system would continue functioning even if nodes within clusters failed.\n",
    "* The data was distributed across all these inexpensive **commodity computers.**\n",
    "* To satisfy a search request, all the computers in the **cluster** searched **in parallel** the portion of the web they had locally.\n",
    "* Then the results of those searches were gathered up and reported back to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.1 Hadoop Overview (cont.)\n",
    "* To accomplish this, Google needed to develop the **clustering hardware and software**, including **distributed storage**.\n",
    "* Google published its designs, but did not open source its software.\n",
    "* Programmers at Yahoo!, working from Google’s designs in the [**“Google File System” paper**](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf), then built their own system.\n",
    "* They **open-sourced** their work and the **Apache organization** implemented the system as **Hadoop**.\n",
    "* The name came from an elephant stuffed animal that belonged to a child of one of Hadoop’s creators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.1 Hadoop Overview (cont.)\n",
    "* Two **additional Google papers** also contributed to the evolution of Hadoop\n",
    "\t* [**“MapReduce: Simplified Data Processing on Large Clusters”**](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) \n",
    "\t* [**“Bigtable: A Distributed Storage System for Structured Data\\”**](http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) \n",
    "\t\t* Basis for Apache HBase (a NoSQL key–value and column-based database).\n",
    "* Many other influential big-data-related papers (including the ones we mentioned) can be found at: \n",
    "> https://bigdata-madesimple.com/research-papers-that-changed-the-world-of-big-data/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS, `MapReduce` and YARN\n",
    "* Hadoop’s key components are: \n",
    "\t* **HDFS (Hadoop Distributed File System)** for storing massive amounts of data throughout a cluster, and \n",
    "\t* **MapReduce** for implementing the tasks that process the data.\n",
    "* **Hadoop MapReduce** is similar in concept to filter/map/reduce in functional-style programming, just on a **massively parallel scale**.\n",
    "* A **MapReduce task** performs two steps—**mapping** and **reduction**.\n",
    "    * The mapping step, which also may include **filtering**, processes the original data across the entire cluster and maps it into tuples of key–value pairs.\n",
    "    * The reduction step then combines those tuples to produce the results of the MapReduce task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS, `MapReduce` and YARN (cont.)\n",
    "* Hadoop divides the data into **batches** that it distributes across the nodes in the cluster—anywhere from a few nodes to a [Yahoo! cluster with 40,000 nodes and over 100,000 cores](https://wiki.apache.org/hadoop/PoweredBy).\n",
    "* Hadoop also distributes the MapReduce task’s code to the nodes in the cluster and executes the code in parallel on every node.\n",
    "* Each node processes only the batch of data stored on that node.\n",
    "* The reduction step combines the results from all the nodes to produce the final result.\n",
    "* To coordinate this, Hadoop uses **YARN (“yet another resource negotiator”)** to manage all the resources in the cluster and schedule tasks for execution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Ecosystem \n",
    "* Hadoop ecosystem includes Spark (discussed in Sections 16.6–16.7) and many other Apache projects:[\\[1\\]](https://hortonworks.com/ecosystems/),[\\[2\\]](https://readwrite.com/2018/06/26/complete-guide-of-hadoop-ecosystem-components/),[\\[3\\]](https://www.janbasktraining.com/blog/introduction-architecture-components-hadoop-ecosystem/)\n",
    "* **Ambari** (`https://ambari.apache.org`)—Tools for managing Hadoop clusters.\n",
    "* **Drill** (`https://drill.apache.org`)—SQL querying of non-relational data in Hadoop and NoSQL databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Ecosystem (cont.)\n",
    "* **Flume** (`https://flume.apache.org`)—A service for collecting and storing (in HDFS and other storage) streaming event data, like high-volume server logs, IoT messages and more. \n",
    "* **HBase** (`https://hbase.apache.org`)—A NoSQL database for big data with \"billions of rows by millions of columns—atop clusters of commodity hardware.\" (We used the word “by” to replace “X” in the original quote.)\n",
    "* **Hive** (`https://hive.apache.org`)—Uses SQL to interact with data in data warehouses. A **data warehouse** aggregates data of various types from various sources. Common operations include extracting data, transforming it and loading (known as **ETL**) into another database, typically so you can analyze it and create reports from it.\n",
    "* **Impala** (`https://impala.apache.org`)—A database for real-time SQL-based queries across distributed data stored in Hadoop HDFS or HBase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Ecosystem (cont.)\n",
    "* **Kafka** (`https://kafka.apache.org`)—Real-time messaging, stream processing and storage, typically to transform and process high-volume streaming data, such as website activity and streaming IoT data.\n",
    "* **Pig** (`https://pig.apache.org`)—A scripting platform that converts data analysis tasks from a scripting language called **Pig Latin** into MapReduce tasks. \n",
    "* **Sqoop* (`https://sqoop.apache.org`)—Tool for moving structured, semi-structured and unstructured data between databases.\n",
    "* **Storm** (`https://storm.apache.org`)—A real-time stream-processing system for tasks such as data analytics, machine learning, ETL and more. \n",
    "* **ZooKeeper** (`https://zookeeper.apache.org`)—A service for managing cluster configurations and coordination between clusters.\n",
    "* And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Providers\n",
    "* Numerous cloud vendors provide **Hadoop as a service**, including Amazon EMR, Google Cloud DataProc, IBM Watson Analytics Engine, Microsoft Azure HDInsight and others.\n",
    "* In addition, companies like **Cloudera** and **Hortonworks** (which at the time of this writing are **merging**) offer integrated Hadoop-ecosystem components and tools via the major cloud vendors.\n",
    "* They also offer **free downloadable environments that you can run on the desktop** for learning, development and testing before you commit to cloud-based hosting, which can incur significant costs. \n",
    "    * Check their significant system requirements first to ensure that you have the disk space and memory required to run them. \n",
    "* We introduce **MapReduce programming** in the example in the following sections by using a **Microsoft cloud-based Azure HDInsight cluster**, which provides **Hadoop as a service**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop 3\n",
    "* **Apache** continues to evolve **Hadoop**.\n",
    "* [Hadoop 3](https://hadoop.apache.org/docs/r3.0.0/) was released in December of 2017 with many improvements, including **better performance** and **significantly improved storage efficiency**.\n",
    "    * https://www.datanami.com/2018/10/18/is-hadoop-officially-dead/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.2 Summarizing Word Lengths in _Romeo and Juliet_ via MapReduce\n",
    "* In the next several subsections, you’ll create a cloud-based, multi-node cluster of computers using Microsoft Azure HDInsight.\n",
    "* Then, you’ll use the service’s capabilities to demonstrate Hadoop MapReduce running on that cluster.\n",
    "* The MapReduce task you’ll define will determine the length of each word in `RomeoAndJuliet.txt` (from the “Natural Language Processing” chapter), then summarize how many words of each length there are.\n",
    "* After defining the task’s mapping and reduction steps, you’ll submit the task to your HDInsight cluster, and Hadoop will decide how to use the cluster of computers to perform the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (cont.)\n",
    "* Most major cloud vendors have support for Hadoop and Spark computing clusters that you can configure to meet your application’s requirements.\n",
    "* Multi-node cloud-based clusters typically are **paid** services, though most vendors provide free trials or credits so you can try out their services.\n",
    "* We want you to experience the process of setting up clusters and using them to perform tasks.\n",
    "* So, in this Hadoop example, you’ll use Microsoft Azure’s HDInsight service to create cloud-based clusters of computers in which to test our examples.\n",
    "* Sign up for an account \n",
    "> https://azure.microsoft.com/en-us/free \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (cont.)\n",
    "* Microsoft requires a credit card for identity verification.\n",
    "* Various services are always free and some you can continue to use for 12 months.\n",
    "* For information on these services see: \n",
    "> https://azure.microsoft.com/en-us/free/free-account-faq/\n",
    "* Microsoft also gives you a credit to experiment with their **paid** services, such as their HDInsight Hadoop and Spark services.\n",
    "* Once your credits run out or 30 days pass (whichever comes first), you cannot continue using paid services unless you authorize Microsoft to charge your card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (cont.)\n",
    "* Because you’ll use your new Azure account’s credit for these examples, we’ll discuss how to configure a low-cost cluster that uses less computing resources than Microsoft allocates by default.\n",
    "* For Microsoft’s latest free account features, visit https://azure.microsoft.com/en-us/free/\n",
    "\t* For Microsoft’s recommended cluster configurations, see https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning#default-node-configuration-and-virtual-machine-sizes-for-clusters. If you configure a cluster that’s too small for a given scenario, when you try to deploy the cluster you’ll receive an error. \n",
    "* **Caution: Once you allocate a cluster, it incurs costs whether you’re using it or not. So, when you complete this case study, be sure to delete your cluster(s) and other resources, so you don’t incur additional charges.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.3 Creating an Apache Hadoop Cluster in Microsoft Azure HDInsight (cont.)\n",
    "For more information, see:\n",
    "> https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-portal\n",
    "* For Azure-related documentation and videos, visit:\n",
    "\t* https://docs.microsoft.com/en-us/azure/—the Azure documentation.\n",
    "\t* https://channel9.msdn.com/—Microsoft’s Channel 9 video network.\n",
    "\t* https://www.youtube.com/user/windowsazure—Microsoft’s Azure channel on YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an HDInsight Hadoop Cluster \n",
    "* The following link explains how to set up a cluster for Hadoop using the Azure HDInsight service:\n",
    "> https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-linux-create-cluster-get-started-portal\n",
    "* While following their **Create a Hadoop cluster** steps, please note the following:\n",
    "\t* In **Step 1**, you access the Azure portal by logging into your account at \n",
    "    > https://portal.azure.com\n",
    "\t* In **Step 3**, you must choose a cluster name that does not already exist. When you enter your cluster name, Microsoft will check whether that name is available and display a message if it is not. You must create a password. For the **Resource group**, you’ll also need to click **Create new** and provide a group name. Leave all other settings in this step as is.\n",
    "\t* In **Step 5**: Under **Select a Storage account**, click **Create new** and provide a storage account name containing only lowercase letters and numbers. Like the cluster name, the storage account name must be unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an HDInsight Hadoop Cluster (cont.)\n",
    "* When you get to the **Cluster summary** you’ll see that Microsoft initially configures the cluster as **Head (2 x D12 v2)**, **Worker (4 x D4 v2)**.\n",
    "* At the time of this writing, the estimated cost-per-hour for this configuration was \\$3.11.\n",
    "* This setup uses a total of 6 CPU nodes with 40 cores—far more than we need for demonstration purposes. \n",
    "* You can edit this setup to use fewer CPUs and cores, which also saves money.\n",
    "* Let’s change the configuration to a four-CPU cluster with 16 cores that uses less powerful computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an HDInsight Hadoop Cluster (cont.)\n",
    "* In the **Cluster summary**: \n",
    "\t1. Click **Edit** to the right of **Cluster size**.\n",
    "\t2. Change the **Number of Worker** nodes to `2`.\n",
    "\t3. Click **Worker node size**, then  **View all**, select **D3 v2** (this is the minimum CPU size for Hadoop nodes) and click **Select**. \n",
    "\t4. Click **Head node size**, then **View all**, select **D3 v2** and click **Select**. \n",
    "\t5. Click **Next** and click **Next** again to return to the **Cluster summary**. Microsoft will validate the new configuration.\n",
    "\t6. When the **Create** button is enabled, click it to deploy the cluster. \n",
    "* It takes 20–30 minutes for Microsoft to “spin up your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an HDInsight Hadoop Cluster (cont.)\n",
    "* During this time, Microsoft is allocating all the resources and software the cluster requires.\n",
    "* After the changes above, our estimated cost for the cluster was \\$1.18 per hour, based on average use for similarly configured clusters.\n",
    "* Our actual charges were less than that.\n",
    "* If you encounter any problems configuring your cluster, [Microsoft provides HDInsight chat-based support](https://azure.microsoft.com/en-us/resources/knowledge-center/technical-chat/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.4 Hadoop Streaming\n",
    "* Hadoop is **Java-based**\n",
    "* Languages like **Python** that are **not natively supported** must use **Hadoop streaming** \n",
    "* **Python MapReduce scripts** communicate with Hadoop via **redirected standard I/O streams**\n",
    "\t* Hadoop redirects **input** to **mapper** script, which reads input from **standard input stream**\n",
    "\t* **Mapper** writes results to **standard output stream**\n",
    "\t* Hadoop redirects **mapper’s output** as **input** to **reducer** script, which reads from the **standard input stream**\n",
    "\t* **Reducer** writes results to **standard output stream** \n",
    "\t* Hadoop writes **reducer’s output** to **HDFS**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.5 Implementing the Mapper \n",
    "* **Mapper** takes **lines of text** as input and **maps** them to **key–value pairs**, each containing a **word length** and **`1`**\n",
    "* **Reducer** will total these **key–value pairs** by key\n",
    "* **Hadoop streaming** expects **mapper’s output** and **reducer’s input/output** to have the form \n",
    "> **_key_`\\t`_value_**\n",
    "* In **`length_mapper.py`**, `#!` tells Hadoop to use Python 3\n",
    "    * Must be first line in the file\n",
    "    * **HDInsight** currently includes **Python 2.7.12** and **Python 3.5.2** \n",
    "    * **Cannot use f-strings** which are **Python 3.6+**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.5 Implementing the Mapper (cont.)\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# length_mapper.py\n",
    "\"\"\"Maps lines of text to key-value pairs of word lengths and 1.\"\"\"\n",
    "import sys\n",
    "\n",
    "def tokenize_input():  # generator function\n",
    "    \"\"\"Split each line of standard input into a list of strings.\"\"\"\n",
    "    for line in sys.stdin:\n",
    "        yield line.split()  \n",
    "\n",
    "# read each line in the the standard input and for every word \n",
    "# produce a key-value pair containing the word, a tab and 1\n",
    "for line in tokenize_input():\n",
    "    for word in line:\n",
    "        print(str(len(word)) + '\\t1')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.6 Implementing the Reducer \n",
    "* In **`length_reducer.py`**, function **`tokenize_input`** is a **generator function** that reads and splits the **key–value pairs** produced by the mapper\n",
    "* **MapReduce streaming** supplies the **standard input**\n",
    "* **`groupby` function** (**`itertools` module**) groups inputs by their **keys** (the **word lengths**) \n",
    "* **Total** all the **counts** for a given **key**\n",
    "* Output a new **key–value pair** consisting of the **word length** and its **total**\n",
    "* **MapReduce** takes the **final word-count outputs** and **writes** them to a file in **HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python3\n",
    "# length_reducer.py\n",
    "\"\"\"Counts the number of words with each length.\"\"\"\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def tokenize_input():\n",
    "    \"\"\"Split each line of standard input into a key and a value.\"\"\"\n",
    "    for line in sys.stdin:\n",
    "        yield line.strip().split('\\t')\n",
    "\n",
    "# produce key-value pairs of word lengths and counts separated by tabs\n",
    "for word_length, group in groupby(tokenize_input(), itemgetter(0)):\n",
    "    try:\n",
    "        total = sum(int(count) for word_length, count in group)\n",
    "        print(word_length + '\\t' + str(total))\n",
    "    except ValueError:\n",
    "        pass  # ignore word if its count was not an integer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.7 Preparing to Run the MapReduce Example\n",
    "* Next, you’ll upload files to the cluster so you can execute the example.\n",
    "* In a Command Prompt, Terminal or shell, change to the folder containing your mapper and reducer scripts and the `RomeoAndJuliet.txt` file.\n",
    "* We assume all three are in this chapter’s `ch16` examples folder, so be sure to copy your `RomeoAndJuliet.txt` file to this folder first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying the Script Files to the HDInsight Hadoop Cluster\n",
    "* Enter the following command to upload the files.\n",
    "* Be sure to replace **YourClusterName** with the cluster name you specified when setting up the Hadoop cluster and press **Enter** only after you’ve typed the entire command.\n",
    "* The colon in the following command is required and indicates that you’ll supply your cluster password when prompted.\n",
    "* At that prompt, type the password you specified when setting up the cluster, then press **Enter**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scp length_mapper.py length_reducer.py RomeoAndJuliet.txt\n",
    "    sshuser@YourClusterName-ssh.azurehdinsight.net:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first time you do this, you’ll be asked for security reasons to confirm that you trust the target host (that is, Microsoft Azure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying RomeoAndJuliet into the Hadoop File System\n",
    "* For Hadoop to read the contents of `RomeoAndJuliet.txt` and supply the lines of text to your mapper, you must first copy the file into Hadoop’s file system.\n",
    "* First, you must use `ssh` to log into your cluster and access its command line. \n",
    "\t* Windows users: If ssh does not work for you, install and enable it as described at https://blogs.msdn.microsoft.com/powershell/2017/12/15/using-the-openssh-beta-in-windows-10-fall-creators-update-and-windows-server-1709/\n",
    "* After completing the installation, log out and log back in or restart your system to enable `ssh`. \n",
    "* In a Command Prompt, Terminal or shell, execute the following command.\n",
    "* Be sure to replace **YourClusterName** with your cluster name.\n",
    "* Again, you’ll be prompted for your cluster password:\n",
    "\n",
    "```\n",
    "ssh sshuser@YourClusterName-ssh.azurehdinsight.net \n",
    "```\n",
    "\n",
    "\n",
    "* For this example, we’ll use the following Hadoop command to copy the text file into the already existing folder `/examples/data` that the cluster provides for use with Microsoft’s Azure Hadoop tutorials.\n",
    "* Again, press **Enter** only when you’ve typed the entire command:\n",
    "\n",
    "```\n",
    "hadoop fs -copyFromLocal RomeoAndJuliet.txt \n",
    "    /example/data/RomeoAndJuliet.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.8 Running the MapReduce Job \n",
    "* Once files are in the cluster, **run MapReduce job** for **`RomeoAndJuliet.txt`** on your **cluster** by executing the following command in the cluster\n",
    "    * You can copy/paste the command from `yarn.txt` located with this example\n",
    "    * We reformatted the command here for readability: \n",
    "\n",
    "```\n",
    "yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \n",
    "   -D mapred.output.key.comparator.class=\n",
    "      org.apache.hadoop.mapred.lib.KeyFieldBasedComparator    \n",
    "   -D mapred.text.key.comparator.options=-n   \n",
    "   -files length_mapper.py,length_reducer.py    \n",
    "   -mapper length_mapper.py \n",
    "   -reducer length_reducer.py    \n",
    "   -input /example/data/RomeoAndJuliet.txt    \n",
    "   -output /example/wordlengthsoutput    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.8 Running the MapReduce Job (cont.)\n",
    "* The **`yarn` command** invokes **Hadoop’s YARN** (“yet another resource negotiator”) tool to manage and coordinate access to the Hadoop resources the **MapReduce task** uses\n",
    "* **`hadoop-streaming.jar`** contains the Java-based **Hadoop streaming utility** that allows you to use Python to implement the **mapper** and **reducer**\n",
    "* The two **`-D` options** set Hadoop properties that enable it to \n",
    "    * **sort the final key–value pairs by key** (**`KeyFieldBasedComparator`**) \n",
    "    * in **numerically (`-n`)** rather than alphabetically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.8 Running the MapReduce Job (cont.)\n",
    "* Other command-line arguments:\n",
    "\t* **`-files`**—Comma-separated list of scripts that Hadoop copies to every node in the cluster so they can **execute locally on each node**\n",
    "\t* **`-mapper`**—mapper’s script file\n",
    "\t* **`-reducer`**—reducer’s script file\n",
    "\t* **`-input`**—**File** or **directory of files** to supply as **mapper input**\n",
    "\t* **`-output`**—**HDFS directory** where final results will be stored\n",
    "        * **Error** if this folder **already exists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5.8 Running the MapReduce Job (cont.)\n",
    "* Sample **output** shows some **Hadoop feedback** produced as the **MapReduce job executes**\n",
    "    * Used **`...`** to save space \n",
    "* Several lines of interest:\n",
    "\t* The total number of **“input paths to process”**—the **`1` source of input** in this example is **`RomeoAndJuliet.txt`**\n",
    "\t* The **“number of splits”**—`2` in this example, based on the **number of worker nodes** in our **HDInsight cluster**\n",
    "\t* The **percentage completion** dates and times&mdash;**big data jobs** could take minutes, hours, days, ...\n",
    "\t* **`File System Counters`** showing numbers of **bytes read and written**\n",
    "\t* **`Job Counters`** showing the **numbers of mapping and reduction tasks used** \n",
    "\t* **`Map-Reduce Framework`** showing stats about the **steps performed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "<pre>\n",
    "packageJobJar: [] [/usr/hdp/2.6.5.3004-13/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.5.3004-13.jar] /tmp/streamjob2764990629848702405.jar tmpDir=null\n",
    "...\n",
    "18/12/05 16:46:25 INFO mapred.FileInputFormat: <strong>Total input paths to process : 1</strong>\n",
    "18/12/05 16:46:26 INFO mapreduce.JobSubmitter: <strong>number of splits:2</strong>\n",
    "...\n",
    "18/12/05 16:46:26 INFO mapreduce.Job: The url to track the job: http://hn0-paulte.y3nghy5db2kehav5m0opqrjxcb.cx.internal.cloudapp.net:8088/proxy/application_1543953844228_0025/\n",
    "...\n",
    "18/12/05 16:46:35 INFO mapreduce.Job:  <strong>map 0% reduce 0%</strong>\n",
    "18/12/05 16:46:43 INFO mapreduce.Job:  <strong>map 50% reduce 0%</strong>\n",
    "18/12/05 16:46:44 INFO mapreduce.Job:  <strong>map 100% reduce 0%</strong>\n",
    "18/12/05 16:46:48 INFO mapreduce.Job:  <strong>map 100% reduce 100%</strong>\n",
    "18/12/05 16:46:50 INFO mapreduce.Job: Job job_1543953844228_0025 <strong>completed successfully</strong>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "18/12/05 16:46:50 INFO mapreduce.Job: Counters: 49\n",
    "\t<strong>File System Counters</strong>\n",
    "\t\tFILE: Number of bytes read=156411\n",
    "\t\tFILE: Number of bytes written=813764\n",
    "...\n",
    "\t<strong>Job Counters</strong> \n",
    "\t\tLaunched map tasks=2\n",
    "\t\tLaunched reduce tasks=1\n",
    "...\n",
    "\t<strong>Map-Reduce Framework</strong>\n",
    "\t\tMap input records=5260\n",
    "\t\tMap output records=25956\n",
    "\t\tMap output bytes=104493\n",
    "\t\tMap output materialized bytes=156417\n",
    "\t\tInput split bytes=346\n",
    "\t\tCombine input records=0\n",
    "\t\tCombine output records=0\n",
    "\t\tReduce input groups=19\n",
    "\t\tReduce shuffle bytes=156417\n",
    "\t\tReduce input records=25956\n",
    "\t\tReduce output records=19\n",
    "\t\tSpilled Records=51912\n",
    "\t\tShuffled Maps =2\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=2\n",
    "\t\tGC time elapsed (ms)=193\n",
    "\t\tCPU time spent (ms)=4440\n",
    "\t\tPhysical memory (bytes) snapshot=1942798336\n",
    "\t\tVirtual memory (bytes) snapshot=8463282176\n",
    "\t\tTotal committed heap usage (bytes)=3177185280\n",
    "...\n",
    "18/12/05 16:46:50 INFO streaming.StreamJob: Output directory: /example/wordlengthsoutput\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Word Counts\n",
    "* **Hadoop MapReduce** saves its output into **HDFS**\n",
    "* To **view final word counts** you must look at the **file in cluster's HDFS**\n",
    "> ```\n",
    "hdfs dfs -text /example/wordlengthsoutput/part-00000\n",
    "```\n",
    "\n",
    "```\n",
    "18/12/05 16:47:19 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library\n",
    "18/12/05 16:47:19 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev b5efb3e531bc1558201462b8ab15bb412ffa6b89]\n",
    "1\t4699\n",
    "2\t3869\n",
    "3\t5651\n",
    "4\t3668\n",
    "5\t2719\n",
    "6\t1624\n",
    "7\t1140\n",
    "8\t1062\n",
    "9\t855\n",
    "10\t317\n",
    "11\t189\n",
    "12\t95\n",
    "13\t35\n",
    "14\t13\n",
    "15\t9\n",
    "16\t6\n",
    "17\t3\n",
    "18\t1\n",
    "23\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Deleting Your Cluster So You Do Not Incur Charges\n",
    "* **Be sure to delete your cluster(s) and associated resources (like storage) so you don’t incur additional charges** \n",
    "    * For details on deleting the cluster, see my [**Python Fundamentals LiveLessons videos**](https://learning.oreilly.com/videos/python-fundamentals/9780135917411) (big data lesson coming soon) and in [**Python for Programmers, Section 16.5.8**](https://learning.oreilly.com/library/view/python-for-programmers/9780135231364/ch16.xhtml#ch16lev2sec23)\n",
    "* [More information](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-portal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;1992&ndash;2020 by Pearson Education, Inc. All Rights Reserved. This content is based on Chapter 5 of the book [**Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud**](https://amzn.to/2VvdnxE).\n",
    "\n",
    "DISCLAIMER: The authors and publisher of this book have used their \n",
    "best efforts in preparing the book. These efforts include the \n",
    "development, research, and testing of the theories and programs \n",
    "to determine their effectiveness. The authors and publisher make \n",
    "no warranty of any kind, expressed or implied, with regard to these \n",
    "programs or to the documentation contained in these books. The authors \n",
    "and publisher shall not be liable in any event for incidental or \n",
    "consequential damages in connection with, or arising out of, the \n",
    "furnishing, performance, or use of these programs.                  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
