{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset \n",
    "* **IMDb (the Internet Movie Database) movie reviews dataset** \n",
    "    * Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher, \"Learning Word Vectors for Sentiment Analysis,\" _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, June 2011. Portland, Oregon, USA. Association for Computational Linguistics, pp. 142–150. http://www.aclweb.org/anthology/P11-1015.\n",
    "* Perform **binary classification** to **predict** whether a review’s **sentiment** is **positive** or **negative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (cont.)\n",
    "* **Recurrent neural networks (RNNs)** process **sequences of data**\n",
    "    * time series\n",
    "    * text in sentences\n",
    "* **“Recurrent”** because the **neural network contains loops**\n",
    "    * **Output of a given layer** becomes the **input to that same layer** in the **next time step**\n",
    "* **Time step**\n",
    "    * **Next point in time** for a **time series**\n",
    "    * **Next word in a sequence of words** for a **text sequence**\n",
    "* **Loops in RNNs** help them **learn relationships** among data in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (cont.)\n",
    "* **“Good”** on its own has **positive sentiment**\n",
    "* **“Not good”** has **negative sentiment** \n",
    "    * **“not”** is **earlier** in the sequence \n",
    "* **RNNs** take into account the **relationships** among **earlier** and **later** data in a sequence\n",
    "* Here, the words that determined sentiment were adjacent\n",
    "* When determining text's meaning, there can be **many words to consider** and an **arbitrary number of words between them**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.9 Recurrent Neural Networks for Sequences; Sentiment Analysis with the IMDb Dataset (cont.)\n",
    "* **Long Short-Term Memory (LSTM)** layer makes the neural network **recurrent** \n",
    "* Optimized to handle **learning from sequences**\n",
    "* RNNs have been used for many tasks including:[\\[1\\]](https://www.analyticsindiamag.com/overview-of-recurrent-neural-networks-and-their-applications/),[\\[2\\]](https://en.wikipedia.org/wiki/Recurrent_neural_network#Applications),[\\[3\\]](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "    * **predictive text input**—displaying possible next words as you type,\n",
    "    * **sentiment analysis**\n",
    "    * **responding to questions with predicted best answers** from a corpus\n",
    "    * **inter-language translation**\n",
    "    * **automated video closed captioning** &mdash; **speech recognition**\n",
    "    * **speech synthesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.1 Loading the IMDb Movie Reviews Dataset \n",
    "* Contains **25,000 training samples** and **25,000 testing samples**, each **labeled** with its positive (1) or negative (0) sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Over 88,000 unique words** in the dataset\n",
    "* Can specify **number of unique words to import** when loading **training and testing data**\n",
    "* We'll use top **10,000 most frequently occurring words** \n",
    "    * Due to **system memory limitations** and **training on a CPU** (intentionally)\n",
    "    * Most people don't have systems with Tensorflow-compatible **GPUs** or **TPUs**\n",
    "* **More data** takes **longer to train**, but may produce **better models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.1 Loading the IMDb Movie Reviews Dataset (cont.)\n",
    "* **`load_data`** **replaces** any words **outside the top 10,000** with a **placeholder** value (discussed shortly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Following cell was added to work around a **known issue with TensorFlow/Keras and NumPy at the time we created the slides**&mdash;this issue is already fixed in a forthcoming version. [See this cell's code on StackOverflow.](https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(\n",
    "    num_words=number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell completes the workaround mentioned above\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.2 Data Exploration (cont.)\n",
    "* Check sample and target dimensions\n",
    "* **Note that `X_train` and `X_test` appear to be one-dimensional**\n",
    "    * They're actually **NumPy arrays of objects** (lists of integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.2 Data Exploration (cont.)\n",
    "* The **arrays `y_train` and `y_test`** are **one-dimensional** arrays containing **1s and 0s**, indicating whether each review is **positive** or **negative**\n",
    "* `X_train` and `X_test` are **lists** of integers, each representing one review’s contents\n",
    "* **Keras models require numeric data** &mdash; **IMDb dataset is preprocessed for you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint  # toggle pretty printing, so elements don't display vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 307, 5, 1301, 20, 1026, 2511, 87, 2775, 52, 116, 5, 31, 7, 4, 91, 1220, 102, 13, 28, 110, 11, 6, 137, 13, 115, 219, 141, 35, 221, 956, 54, 13, 16, 11, 2714, 61, 322, 423, 12, 38, 76, 59, 1803, 72, 8, 2, 23, 5, 967, 12, 38, 85, 62, 358, 99]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Encodings \n",
    "* Because the **movie reviews** are **numerically encoded**, to view their original text, you need to know the word to which each number corresponds\n",
    "* **Keras’s IMDb dataset** provides a **dictionary** that **maps the words to their indexes**\n",
    "* **Each word’s value** is its **frequency ranking** among all words in the dataset\n",
    "    * **Ranking 1** is the **most frequently occurring word**\n",
    "    * **Ranking 2** is the **second most frequently occurring word**\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Encodings (cont.)\n",
    "* Ranking values are **offset by 3** in the training/testing samples\n",
    "    * **Most frequently occurring word has the value 4** wherever it appears in a review\n",
    "* **0, 1 and 2** in each encoded review are **reserved**:\n",
    "    * **padding (0)** \n",
    "        * All training/testing samples **must have same dimensions**\n",
    "        * Some reviews may need to be padded with **0** and some shortened\n",
    "    * **start of a sequence (1)** &mdash; a **token** that Keras uses internally for learning purposes\n",
    "    * **unknown word (2)** &mdash; typically a word that was **not loaded**\n",
    "        * **`load_data`** uses **2** for words with **frequency rankings greater than `num_words`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review \n",
    "* Must account for offset when **decoding reviews**\n",
    "* Get the **word-to-index dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The word `'great'` might appear in a positive movie review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['great']  # 84th most frequent word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review (cont.)\n",
    "* **Reverse `word_to_index` mapping**, so we can **look up words** by **frequency rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for (word, index) in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Top 50 words**—**most frequent word** has the key **1** in the **new dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'are', 'his', 'have', 'he', 'be', 'one', 'all', 'at', 'by', 'an', 'they', 'who', 'so', 'from', 'like', 'her', 'or', 'just', 'about', \"it's\", 'out', 'has', 'if', 'some', 'there', 'what', 'good', 'more']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index_to_word[i] for i in range(1, 51)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding a Movie Review (cont.)\n",
    "* Now, we can **decode a review**\n",
    "* **`i - 3`** accounts for the **frequency ratings offsets** in the encoded reviews \n",
    "* For `i` values `0`–`2`, `get` returns `'?'`; otherwise, `get` returns the word with the **key `i - 3`** in the **`index_to_word` dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? beautiful and touching movie rich colors great settings good acting and one of the most charming movies i have seen in a while i never saw such an interesting setting when i was in china my wife liked it so much she asked me to ? on and rate it so other would enjoy too'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([index_to_word.get(i - 3, '?') for i in X_train[123]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can see from **`y_train[123]`** that this **review** is **classified as positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.3 Data Preparation \n",
    "* Number of words per review varies\n",
    "* Keras **requires all samples to have the same dimensions**\n",
    "* **Prepare data** for learning\n",
    "\t* Restrict every review to the **same number of words**\n",
    "\t* **Pad** some with **0s**, **truncate** others\n",
    "* **`pad_sequences` function** reshapes samples and **returns a 2D array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_review = 200  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=words_per_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.3 Data Preparation (cont.)\n",
    "* Must also **reshape `X_test`** for evaluating the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, maxlen=words_per_review) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Test Data into Validation and Test Data\n",
    "* Split the **25,000 test samples** into **20,000 test samples** and **5,000 validation samples**\n",
    "* We'll pass validation samples to the model’s `fit` method via **`validation_data`** argument\n",
    "* Use **Scikit-learn’s `train_test_split` function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, random_state=11, test_size=0.20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confirm the split by checking `X_test`’s and `X_val`’s shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.4 Creating the Neural Network\n",
    "* Begin with a **`Sequential` model** and import the other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Embedding Layer \n",
    "* Our convnet example used **one-hot encoding** to convert the **MNIST’s integer labels** into **categorical** data\n",
    "    * **Result for each label** was a **vector** in which **all but one element was 0**\n",
    "* Could do that for index values that represent words, but with **10,000 unique words**:\n",
    "\t* Need a **10,000-by-10,000 array** to represent all words\n",
    "\t* **100,000,000 elements** and **almost all** would be **0**\n",
    "\t* For **all 88,000+ unique words** in the dataset, need nearly **eight billion elements**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Embedding Layer (cont.)\n",
    "* To **reduce dimensionality**, RNNs that process **text sequences** typically begin with an **embedding layer** \n",
    "* Encodes each word in a more compact **dense-vector representation**\n",
    "* These capture the **word’s context**—how a given word **relates to words around it**\n",
    "* Help **RNN learn word relationships** \n",
    "* **Predefined word embeddings**, such as **Word2Vec** and **GloVe**\n",
    "\t* Can **load** into neural networks to **save training time**\n",
    "\t* Sometimes used to **add basic word relationships** to a model when **smaller amounts of training data** are available\n",
    "\t* **Improve model accuracy** by **building upon previously learned word relationships**, rather than trying to learn those relationships with insufficient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an `Embedding` Layer (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pauldeitel/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "rnn.add(Embedding(input_dim=number_of_words, output_dim=128,\n",
    "                  input_length=words_per_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`input_dim=number_of_words`**—Number of **unique words**\n",
    "* **`output_dim=128`**—Size of each word embedding\n",
    "    * If you [load pre-existing embeddings](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) like **Word2Vec** and **GloVe**, you must set this to **match the size of the word embeddings you load**\n",
    "* **`input_length=words_per_review`**—Number of words in each input sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pauldeitel/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "rnn.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`units`**—**number of neurons** in the layer\n",
    "\t* **More neurons** means **network can remember more**\n",
    "\t* [**Guideline**](https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046): Value between **length of the sequences** (200 in this example) and **number of classes to predict** (2 in this example)\n",
    "* **`dropout`**—**percentage of neurons to randomly disable** when processing the layer’s input and output\n",
    "\t* Like **pooling layers** in a **convnet**, **dropout** is a proven technique that **reduces overfitting**\n",
    "        * Yarin, Ghahramani, and Zoubin. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.” October 05, 2016. https://arxiv.org/abs/1512.05287\n",
    "        * Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” _Journal of Machine Learning Research_ 15 (June 14, 2014): 1929-1958. http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\t* Keras also provides a **`Dropout`** layer that you can add to your models \n",
    "* **`recurrent_dropout`**—**percentage of neurons to randomly disable** when the **layer’s output** is **fed back into the layer** again to allow the network to **learn from what it has seen previously**\n",
    "    * **Mechanics of how the LSTM layer performs its task are beyond scope**.\n",
    "        * Chollet says: “you don’t need to understand anything about the specific architecture of an LSTM cell; **as a human, it shouldn’t be your job to understand it**. Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time.”\n",
    "\t\t* Chollet, François. _Deep Learning with Python_. p. 204. Shelter Island, NY: Manning Publications, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Dense Output Layer \n",
    "* Reduce the **LSTM layer’s output** to **one result** indicating whether a review is **positive** or **negative**, thus the value **`1` for the `units` argument**\n",
    "* **`'sigmoid`' activation function** is preferred for **binary classification**\n",
    "\t* Chollet, François. _Deep Learning with Python_. p.114. Shelter Island, NY: Manning Publications, 2018.\n",
    "\t* Reduces arbitrary values into the range **0.0–1.0**, producing a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model and Displaying the Summary\n",
    "* **Two possible outputs**, so we use the **`binary_crossentropy` loss function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam',\n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fewer layers** than our **convnet**, but nearly **three times as many parameters** (the network’s **weights**)  \n",
    "\t* **More parameters means more training time**\n",
    "\t* The large number of parameters primarily comes from the **number of words in the vocabulary** (we loaded 10,000) **times the number of neurons in the `Embedding` layer’s output (128)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,411,713\n",
      "Trainable params: 1,411,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.5 Training and Evaluating the Model \n",
    "* For each **epoch** the **RNN model** takes **significantly longer to train** than our **convnet**\n",
    "    * Due to the **larger numbers of parameters** (weights) our **RNN model** needs to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 20000 samples\n",
      "WARNING:tensorflow:From /Users/pauldeitel/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 322s 13ms/sample - loss: 0.4724 - acc: 0.7806 - val_loss: 0.3799 - val_acc: 0.8342\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 298s 12ms/sample - loss: 0.3411 - acc: 0.8574 - val_loss: 0.3446 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 282s 11ms/sample - loss: 0.2696 - acc: 0.8920 - val_loss: 0.3375 - val_acc: 0.8571\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 274s 11ms/sample - loss: 0.2149 - acc: 0.9159 - val_loss: 0.4126 - val_acc: 0.8597\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 254s 10ms/sample - loss: 0.1677 - acc: 0.9374 - val_loss: 0.4108 - val_acc: 0.8607\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 249s 10ms/sample - loss: 0.1284 - acc: 0.9529 - val_loss: 0.4379 - val_acc: 0.8576\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 254s 10ms/sample - loss: 0.1033 - acc: 0.9636 - val_loss: 0.4487 - val_acc: 0.8599\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 246s 10ms/sample - loss: 0.0845 - acc: 0.9715 - val_loss: 0.4942 - val_acc: 0.8597\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 250s 10ms/sample - loss: 0.0716 - acc: 0.9756 - val_loss: 0.4808 - val_acc: 0.8472\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 270s 11ms/sample - loss: 0.0553 - acc: 0.9809 - val_loss: 0.5299 - val_acc: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History object at 0x140abeac8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_train, y_train, epochs=10, batch_size=32, \n",
    "        validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "```\n",
    "Train on 25000 samples, validate on 20000 samples\n",
    "WARNING:tensorflow:From /Users/pauldeitel/anaconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.cast instead.\n",
    "Epoch 1/10\n",
    "25000/25000 [==============================] - 297s 12ms/sample - loss: 0.4827 - acc: 0.7673 - val_loss: 0.3925 - val_acc: 0.8324\n",
    "Epoch 2/10\n",
    "25000/25000 [==============================] - 291s 12ms/sample - loss: 0.3327 - acc: 0.8618 - val_loss: 0.3614 - val_acc: 0.8461\n",
    "Epoch 3/10\n",
    "25000/25000 [==============================] - 272s 11ms/sample - loss: 0.2662 - acc: 0.8937 - val_loss: 0.3503 - val_acc: 0.8492\n",
    "Epoch 4/10\n",
    "25000/25000 [==============================] - 272s 11ms/sample - loss: 0.2066 - acc: 0.9198 - val_loss: 0.3695 - val_acc: 0.8623\n",
    "Epoch 5/10\n",
    "25000/25000 [==============================] - 271s 11ms/sample - loss: 0.1612 - acc: 0.9403 - val_loss: 0.3802 - val_acc: 0.8587\n",
    "Epoch 6/10\n",
    "25000/25000 [==============================] - 291s 12ms/sample - loss: 0.1218 - acc: 0.9556 - val_loss: 0.4103 - val_acc: 0.8421\n",
    "Epoch 7/10\n",
    "25000/25000 [==============================] - 295s 12ms/sample - loss: 0.1023 - acc: 0.9634 - val_loss: 0.4634 - val_acc: 0.8582\n",
    "Epoch 8/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0789 - acc: 0.9732 - val_loss: 0.5103 - val_acc: 0.8555\n",
    "Epoch 9/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0676 - acc: 0.9775 - val_loss: 0.5071 - val_acc: 0.8526\n",
    "Epoch 10/10\n",
    "25000/25000 [==============================] - 273s 11ms/sample - loss: 0.0663 - acc: 0.9787 - val_loss: 0.5156 - val_acc: 0.8536\n",
    "<tensorflow.python.keras.callbacks.History object at 0x141462e48>\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.9.5 Training and Evaluating the Model (cont.)\n",
    "* Function **`evaluate`** returns the **loss and accuracy values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 34s 2ms/sample - loss: 0.5299 - acc: 0.8511\n"
     ]
    }
   ],
   "source": [
    "results = rnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5299076704353094, 0.85115]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Accuracy seems low** compared to our **convnet**, but this is a **much more difficult problem**\n",
    "    * Many **IMDb sentiment-analysis binary-classification studies** show results **in the high 80s**\n",
    "* We did **reasonably well** with our **small recurrent neural network** of only **three layers**\n",
    "    * We have not tried to tune our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "&copy;1992&ndash;2020 by Pearson Education, Inc. All Rights Reserved. This content is based on Chapter 5 of the book [**Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud**](https://amzn.to/2VvdnxE).\n",
    "\n",
    "DISCLAIMER: The authors and publisher of this book have used their \n",
    "best efforts in preparing the book. These efforts include the \n",
    "development, research, and testing of the theories and programs \n",
    "to determine their effectiveness. The authors and publisher make \n",
    "no warranty of any kind, expressed or implied, with regard to these \n",
    "programs or to the documentation contained in these books. The authors \n",
    "and publisher shall not be liable in any event for incidental or \n",
    "consequential damages in connection with, or arising out of, the \n",
    "furnishing, performance, or use of these programs.                  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
